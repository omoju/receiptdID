{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)\n",
    "\n",
    "# This line will hide code by default when the notebook is exported as HTML\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n",
    "\n",
    "# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\n",
    "di.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>''', raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receipt.ID\n",
    "### Hierarchical item classification\n",
    "Taxonomic classification, categorize items according to a pre-defined taxonomy. The goal is to assign one or more categories in the taxonomy to an item. It is a multi-class **and** multi-label classification problem with hierarchical relationships between each node in the tree.\n",
    "\n",
    "#### Items\n",
    "- Items come from a wide range for categories like Produce, Meat, Beverage, Supplies. \n",
    "- Example item to category mapping:\n",
    "\n",
    "\n",
    "|item|mapping|\n",
    "|---|---|\n",
    "|Kale  | \"Food/Produce/Kale\"  |\n",
    "|Vinegar white wine 50 grain  | \"Food/Dry-Grocery/Vinegars/White Wine Vinegar\"  |\n",
    "|Imported nat flank steak  | \"Food/Meats/Beef/Flank Steak\"  |\n",
    "\n",
    "\n",
    "To solve this problem, I will undertake the following course of action:\n",
    "1. Explore the dataset\n",
    "    - Explore the dataset to ensure its integrity and understand the context. \n",
    "2. Identify features that may be used. \n",
    "    - If possible, engineer features that might provide greater discrimination.\n",
    "3. Build **k** independent *text-based* classifiers for the text-based features and feed the output from these classifiers into the next layer classifier which takes in the other features. This approach combines information from both the text-based labels as well as the item’s metadata. Explore a couple of classifiers that might be well suited for the problem at hand.\n",
    "    - RandomForest\n",
    "    - DecisionTree\n",
    "    - SVC\n",
    "    - AdaBoost       \n",
    "\n",
    "4.  Select appropriate classifier based on evaluation metric and tune it for optimality.\n",
    "\n",
    "In this notebook I do processes 3 and 4.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('tools/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Graphing Libraries\n",
    "import matplotlib.pyplot as pyplt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")  \n",
    "\n",
    "# Use CPickle if available\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in a data file with 127108 datapoints\n"
     ]
    }
   ],
   "source": [
    "dataPath = '/Users/omojumiller/mycode/insight/PlateIQ/'\n",
    "df = pd.read_pickle(dataPath+'data/df_data_vectors.dat')\n",
    "df_catergory_lookup = pd.read_pickle(dataPath+'data/data_category_lookup.dat')\n",
    "print ('Read in a data file with {0} datapoints'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def shuffle_split_data(X, y):\n",
    "    \"\"\" Shuffles and splits data into 75% training and 25% testing subsets,\n",
    "        then returns the training and testing subsets. \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=num_train, random_state=42)\n",
    "    \n",
    "    # Return the training and testing data subsets\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    \"\"\"Transforms lists of feature-value mappings to vectors.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    object : word2vec model \n",
    "    \"\"\"\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_categories(level, num_data = 1000):\n",
    "    \"\"\"Gets categories which at least num_data datapoints.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    level : column name\n",
    "    num_data : int\n",
    "    \"\"\"    \n",
    "    the_categories = {}\n",
    "    counts = df.groupby(level).size()\n",
    "    total = df.groupby(level).size().sum()\n",
    "    \n",
    "    for i in counts.index:\n",
    "        if counts[i] > num_data:\n",
    "            the_categories[int(i)] = counts[i]\n",
    "\n",
    "\n",
    "    print(level, 'has', len(the_categories), 'categories with enough data.')  \n",
    "\n",
    "    for key in the_categories:\n",
    "        find_name = df_catergory_lookup.category_id == key\n",
    "        name = df_catergory_lookup[find_name].category_name\n",
    "        the_index = name.index[0]\n",
    "\n",
    "    return the_categories, total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current approach\n",
    "The current approach used focuses exclusively on the item's name, for example, a data point in the dataset would have an object's name as \"mary's organic fryers\" or \"organic baby spinach.\" The first challenge with this approach lay in the fact that the item label was quite short, roughly about two to eight words.  Further, when modifiers like *gluten free*, *organic*, or *pesticide free* wherein the item's label, this added a layer of misinformation causing items like *organic milk* and *organic beer* to be classified in the same class.\n",
    "\n",
    "## Samples\n",
    "I have selected these samples to see that I can correctly separate them even though they have the modifiers in their item name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen samples of items:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mapped_category</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price_stddev</th>\n",
       "      <th>primary_unit</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>category</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>...</th>\n",
       "      <th>catg_986</th>\n",
       "      <th>catg_989</th>\n",
       "      <th>catg_998</th>\n",
       "      <th>catg_999</th>\n",
       "      <th>catg_1002</th>\n",
       "      <th>catg_495</th>\n",
       "      <th>catg_499</th>\n",
       "      <th>catg_1015</th>\n",
       "      <th>catg_1019</th>\n",
       "      <th>catg_1021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>[Food, Produce, Spinach]</td>\n",
       "      <td>1616</td>\n",
       "      <td>0.539695</td>\n",
       "      <td>1</td>\n",
       "      <td>12.6064</td>\n",
       "      <td>[1, 9, 91]</td>\n",
       "      <td>14</td>\n",
       "      <td>organic baby spinach</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258241</td>\n",
       "      <td>0.146029</td>\n",
       "      <td>0.277557</td>\n",
       "      <td>0.010304</td>\n",
       "      <td>0.280883</td>\n",
       "      <td>0.225076</td>\n",
       "      <td>0.314643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64391</th>\n",
       "      <td>[Beverages, Alcoholic, Beers]</td>\n",
       "      <td>508958</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>165</td>\n",
       "      <td>[3, 6, 16]</td>\n",
       "      <td>248</td>\n",
       "      <td>eel river organic 15.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.248294</td>\n",
       "      <td>0.338635</td>\n",
       "      <td>0.339625</td>\n",
       "      <td>0.324124</td>\n",
       "      <td>0.281059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55697</th>\n",
       "      <td>[Food, Meats, Beef, Beef Tongue]</td>\n",
       "      <td>455407</td>\n",
       "      <td>0.0151186</td>\n",
       "      <td>3</td>\n",
       "      <td>2.98429</td>\n",
       "      <td>[1, 7, 70, 1675]</td>\n",
       "      <td>4218</td>\n",
       "      <td>mary's organic fryers</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334703</td>\n",
       "      <td>0.174655</td>\n",
       "      <td>0.284945</td>\n",
       "      <td>0.249411</td>\n",
       "      <td>0.222272</td>\n",
       "      <td>0.272440</td>\n",
       "      <td>0.318668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mapped_category item_id price_stddev primary_unit  \\\n",
       "274            [Food, Produce, Spinach]    1616     0.539695            1   \n",
       "64391     [Beverages, Alcoholic, Beers]  508958         None            4   \n",
       "55697  [Food, Meats, Beef, Beef Tongue]  455407    0.0151186            3   \n",
       "\n",
       "      price_mean          category vendor_id               item_name  level_0  \\\n",
       "274      12.6064        [1, 9, 91]        14    organic baby spinach        1   \n",
       "64391        165        [3, 6, 16]       248  eel river organic 15.5        3   \n",
       "55697    2.98429  [1, 7, 70, 1675]      4218   mary's organic fryers        1   \n",
       "\n",
       "       level_1    ...     catg_986  catg_989  catg_998  catg_999  catg_1002  \\\n",
       "274        9.0    ...     0.258241  0.146029  0.277557  0.010304   0.280883   \n",
       "64391      6.0    ...     0.245825       NaN       NaN       NaN        NaN   \n",
       "55697      7.0    ...     0.334703  0.174655  0.284945  0.249411   0.222272   \n",
       "\n",
       "       catg_495  catg_499 catg_1015 catg_1019 catg_1021  \n",
       "274    0.225076  0.314643       NaN       NaN       NaN  \n",
       "64391  0.248294  0.338635  0.339625  0.324124  0.281059  \n",
       "55697  0.272440  0.318668       NaN       NaN       NaN  \n",
       "\n",
       "[3 rows x 164 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices = [274, 64391, 55697] \n",
    "\n",
    "# Create a DataFrame of the chosen samples\n",
    "samples = pd.DataFrame(df.loc[indices], columns = df.keys())\n",
    "print (\"Chosen samples of items:\")\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## My Approach\n",
    "I took an entirely different approach. I got inspiration from the approach that Google, [YouTube](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36411.pdf) used in organizing videos and decided to shift the unit of analysis from item *name* to the *categories* themselves. My approach combines information from both the text-based labels as well as the item's metadata.\n",
    "\n",
    "This method achieves two crucial things. First, by focusing on individual categories, each time a new category of item is added to the restaurant domain, instead of having to retrain the classifier on the entire dataset, all we have to do is gather enough data for that category, and train a classifier for it. This way, the approach can scale beautifully as the taxonomy grows. Second, moving the unit of analysis from text labels to categories, it becomes easier to correctly separate \"organic cream\" and \"organic beer.\" \n",
    "\n",
    "I chose to discard categories that had less than 300 datapoints. As the datapoints in the category grows, those categories can then been trained individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_0 has 6 categories with enough data.\n"
     ]
    }
   ],
   "source": [
    "the_level = {}\n",
    "num_data = 300\n",
    "\n",
    "level = 'level_0'\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_1 has 19 categories with enough data.\n"
     ]
    }
   ],
   "source": [
    "level = 'level_1'\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_2 has 65 categories with enough data.\n"
     ]
    }
   ],
   "source": [
    "level = 'level_2'\n",
    "df.loc[:,('label')] = df.loc[:, level]\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_3 has 42 categories with enough data.\n"
     ]
    }
   ],
   "source": [
    "level = 'level_3'\n",
    "df.loc[:,('label')] = df.loc[:, level]\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_4 has 9 categories with enough data.\n"
     ]
    }
   ],
   "source": [
    "level = 'level_4'\n",
    "df.loc[:,('label')] = df.loc[:, level]\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 141 categories in all\n"
     ]
    }
   ],
   "source": [
    "k_categories = 0\n",
    "for key in the_level.keys():\n",
    "    k_categories += len(the_level[key].keys())\n",
    "   \n",
    "    \n",
    "print('processed {} categories in all'.format(k_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling\n",
    "For each data point, I assigned the deepest class node in the tree to which it belonged as its label. For example, an item named \"bacon ends\" belonged to both classes [Food], [Food, Meats] and [Food, Meats, Pork]. For such an item, I assigned it is label as \"Pork.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_examples_ = df['label'].isnull()\n",
    "df.ix[df[null_examples_].index, 'label'] = df.loc[:, 'level_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the *k* training sets for the *k* categories for each level in the tree   \n",
    "The aim of moving to a category-based solution is to embed knowledge of the taxonomy into classifiers. To do this, I had to figure out how to get positive and negative samples for each category. For every category node, I decided that itself, as well as all its descendants, were *positive* samples for that class. All other nodes that were neither the categories ancestor(s) or itself were set as *negative* samples.  The figure below gives a visual explanation of selecting category training set. I did this for each category node in the tree that had enough data.\n",
    "<img src=\"images/hc_5.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "One of the limitations of this method is that most nodes have un-balanced classes. Some more severe than other, especially as you go further down in the tree. There are some classes whose ratio of positive signals is as small as 0.02%. The more granular that subclasses get, the harder it is to classify them. The good news is that one can decide to focus on a few of these classes and use synthetic methods to rebalance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dict of positive and negative samples for the categories\n",
    "category_samples = {}\n",
    "category_positive_samples = {}\n",
    "category_negative_samples= {}\n",
    "category_test_samples = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1: Food\n",
      "processing 2: Supplies\n",
      "processing 3: Beverages\n",
      "processing 4: Other\n",
      "processing 495: Grocery\n",
      "processing 499: Protein\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_0'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "    \n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_1'].isnull()\n",
    "    positive_examples = df['level_1'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_samples[key] = df[key_df & null_examples]\n",
    "        category_samples[key] = category_samples[key].append(df[key_df & positive_examples])\n",
    "        num_test = (len(category_samples[key]) // 10) * 2\n",
    "        category_positive_samples[key] = category_samples[key][num_test:]\n",
    "        category_test_samples[key] = category_samples[key][:num_test]        \n",
    "\n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    level_0_item = df['level_0'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1281: SF Checkout Bag Fee\n",
      "processing 130: Cream\n",
      "processing 375: Sausages\n",
      "processing 132: Tomatoes\n",
      "processing 134: Peppers\n",
      "processing 391: Salt\n",
      "processing 392: Disposables & Packaging Supplies\n",
      "processing 267: Glasses\n",
      "processing 258: Keg Deposit\n",
      "processing 143: Tortillas\n",
      "processing 16: Beers\n",
      "processing 145: Cucumbers\n",
      "processing 18: Wines\n",
      "processing 531: Condiments\n",
      "processing 535: Fruits\n",
      "processing 152: Flour & Starch\n",
      "processing 537: Herbs\n",
      "processing 26: Juices\n",
      "processing 157: Garlic\n",
      "processing 1030: Seeds\n",
      "processing 40: Teas\n",
      "processing 44: Beans\n",
      "processing 482: Linens\n",
      "processing 177: Onions\n",
      "processing 51: Fuel & Freight/Delivery\n",
      "processing 1077: Breads\n",
      "processing 54: Liquor\n",
      "processing 55: Oils\n",
      "processing 56: Pork\n",
      "processing 1730: Leaves\n",
      "processing 325: Eggs\n",
      "processing 70: Beef\n",
      "processing 456: Sodas\n",
      "processing 329: Poultry\n",
      "processing 74: Fish\n",
      "processing 331: Coffee\n",
      "processing 76: Spices\n",
      "processing 226: Vinegars\n",
      "processing 78: Cheese\n",
      "processing 464: Canned\n",
      "processing 1272: Squash\n",
      "processing 84: Lettuce\n",
      "processing 1365: Spreads & Pastes\n",
      "processing 86: Asparagus\n",
      "processing 87: Syrup\n",
      "processing 216: Radishes\n",
      "processing 89: Kale\n",
      "processing 986: Cleaning & Janitorial\n",
      "processing 1039: Nuts & Grains\n",
      "processing 98: Beets\n",
      "processing 228: Sugar\n",
      "processing 1041: Medical Supplies\n",
      "processing 105: Lamb\n",
      "processing 235: Chocolate\n",
      "processing 109: Broccoli\n",
      "processing 111: Mushrooms\n",
      "processing 1960: Glass Cleaner\n",
      "processing 115: Butter \n",
      "processing 118: Potatoes\n",
      "processing 119: Cabbages\n",
      "processing 332: Pasta\n",
      "processing 1078: Shellfish\n",
      "processing 124: Milk\n",
      "processing 125: Carrots\n",
      "processing 126: Cauliflower\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_2'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_3'].isnull()\n",
    "    positive_examples = df['level_3'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_samples[key] = df[key_df & null_examples]\n",
    "        category_samples[key] = category_samples[key].append(df[key_df & positive_examples])\n",
    "        num_test = (len(category_samples[key]) // 10) * 2\n",
    "        category_positive_samples[key] = category_samples[key][num_test:]\n",
    "        category_test_samples[key] = category_samples[key][:num_test]        \n",
    "    \n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    find_level_0 = df[level] == key\n",
    "    name_level_0 = df[find_level_0].level_0\n",
    "    the_index_level_0 = name_level_0.index[0]\n",
    "    \n",
    "    find_level_1 = df[level] == key\n",
    "    name_level_1 = df[find_level_1].level_1\n",
    "    the_index_level_1 = name_level_1.index[0]\n",
    "    \n",
    "    level_0_item = df['level_0'] != name_level_0.ix[the_index_level_0]\n",
    "    level_1_item = df['level_1'] != name_level_1.ix[the_index_level_1]\n",
    "    level_2_item = df['level_2'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item & level_1_item & level_2_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 387: Oysters\n",
      "processing 136: Cilantro\n",
      "processing 1033: Ground Black Pepper\n",
      "processing 1802: Baby Carrot\n",
      "processing 1239: Lids\n",
      "processing 275: Containers\n",
      "processing 148: Olives\n",
      "processing 25: Vodka\n",
      "processing 282: Uniforms\n",
      "processing 286: Bags\n",
      "processing 32: Gin\n",
      "processing 289: Cups\n",
      "processing 164: Mints\n",
      "processing 39: Tequila\n",
      "processing 176: Oranges\n",
      "processing 1160: Red Wines\n",
      "processing 50: Iced Tea\n",
      "processing 52: Whiskey\n",
      "processing 437: Bacon\n",
      "processing 1209: Gloves\n",
      "processing 319: Red Onions\n",
      "processing 963: Mozzarella\n",
      "processing 69: Chicken\n",
      "processing 970: Extra Virgin Olive Oil\n",
      "processing 1200: Shrimp\n",
      "processing 1358: Rice\n",
      "processing 80: Apples\n",
      "processing 355: Duck\n",
      "processing 980: Dried Fruits\n",
      "processing 983: Cheddar\n",
      "processing 88: Avocados\n",
      "processing 1113: Kegs\n",
      "processing 95: Basil\n",
      "processing 99: Bell Peppers\n",
      "processing 229: Sauces\n",
      "processing 998: White Wines\n",
      "processing 363: Rum\n",
      "processing 403: Purees\n",
      "processing 247: Zucchini Squash\n",
      "processing 1019: Salmon\n",
      "processing 1148: Liqueur\n",
      "processing 1021: Tuna\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_3'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_4'].isnull()\n",
    "    positive_examples = df['level_4'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_samples[key] = df[key_df & null_examples]\n",
    "        category_samples[key] = category_samples[key].append(df[key_df & positive_examples])\n",
    "        num_test = (len(category_samples[key]) // 10) * 2\n",
    "        category_positive_samples[key] = category_samples[key][num_test:]\n",
    "        category_test_samples[key] = category_samples[key][:num_test]        \n",
    "  \n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    find_level_0 = df[level] == key\n",
    "    name_level_0 = df[find_level_0].level_0\n",
    "    the_index_level_0 = name_level_0.index[0]\n",
    "    \n",
    "    find_level_1 = df[level] == key\n",
    "    name_level_1 = df[find_level_1].level_1\n",
    "    the_index_level_1 = name_level_1.index[0]\n",
    "    \n",
    "    find_level_2 = df[level] == key\n",
    "    name_level_2 = df[find_level_2].level_1\n",
    "    the_index_level_2 = name_level_2.index[0]\n",
    "    \n",
    "    level_0_item = df['level_0'] != name_level_0.ix[the_index_level_0]\n",
    "    level_1_item = df['level_1'] != name_level_1.ix[the_index_level_1]\n",
    "    level_2_item = df['level_2'] != name_level_2.ix[the_index_level_2]\n",
    "    level_3_item = df['level_3'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item & level_1_item & level_2_item & level_3_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 35: Bourbon\n",
      "processing 999: Cabernet Sauvignon\n",
      "processing 1064: Chardonnay\n",
      "processing 1002: Sauvignon Blanc\n",
      "processing 1164: Brut\n",
      "processing 49: Rye\n",
      "processing 21: Rose Wine\n",
      "processing 1015: Chicken Breast\n",
      "processing 989: Pinot Noir\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_4'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_5'].isnull()\n",
    "    positive_examples = df['level_5'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_samples[key] = df[key_df & null_examples]\n",
    "        category_samples[key] = category_samples[key].append(df[key_df & positive_examples])\n",
    "        num_test = (len(category_samples[key]) // 10) * 2\n",
    "        category_positive_samples[key] = category_samples[key][num_test:]\n",
    "        category_test_samples[key] = category_samples[key][:num_test]        \n",
    "\n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    find_level_0 = df[level] == key\n",
    "    name_level_0 = df[find_level_0].level_0\n",
    "    the_index_level_0 = name_level_0.index[0]\n",
    "    \n",
    "    find_level_1 = df[level] == key\n",
    "    name_level_1 = df[find_level_1].level_1\n",
    "    the_index_level_1 = name_level_1.index[0]\n",
    "    \n",
    "    find_level_2 = df[level] == key\n",
    "    name_level_2 = df[find_level_2].level_1\n",
    "    the_index_level_2 = name_level_2.index[0]\n",
    "    \n",
    "    find_level_3 = df[level] == key\n",
    "    name_level_3 = df[find_level_3].level_1\n",
    "    the_index_level_3 = name_level_3.index[0]\n",
    "    \n",
    "    level_0_item = df['level_0'] != name_level_0.ix[the_index_level_0]\n",
    "    level_1_item = df['level_1'] != name_level_1.ix[the_index_level_1]\n",
    "    level_2_item = df['level_2'] != name_level_2.ix[the_index_level_2]\n",
    "    level_3_item = df['level_3'] != name_level_3.ix[the_index_level_3]\n",
    "    level_4_item = df['level_4'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item & level_1_item & level_2_item & level_3_item & level_4_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "With the training sets done, I vectorized them using a mean embedding vectorizer. For each category, I created an AdaBoost binary classifier. I chose to use the AdaBoost after I experimented with several classifiers like LinearSVM and Decision Trees. I first selected Decision Tree since it outperformed the LinearSVM for this problem. However, I quickly realized my aim was to get predicted probabilities as output for the class instead of simple binary output which the Decision Tree gave. For that reason, I switched to AdaBoost, which is a *boosted* decision tree. \n",
    "\n",
    "I train each classifier using a k=5 kfold cross-validation scheme. I fitted the resulting classifier and retrieved the predicted probability for each data point in the dataset, which resulted in *k* vectors for *k* categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process item label names using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Only use item labels as input into word2vec embeddings\n",
    "# train word2vec on all the item_labels \n",
    "\n",
    "w2v_model = Word2Vec(df['item_labels'], size=750, window=5, min_count=5, workers=4)\n",
    "w2v = dict(zip(w2v_model.index2word, w2v_model.syn0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True) #to trim unneeded model memory = use (much) less RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train *k* classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time, gmtime, strftime\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "seed = 342 # For reproducability\n",
    "Ada_w2v =  Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), (\"Ada\", AdaBoostClassifier(n_estimators=10))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(dataPath+'data/df_category_scores.pkl') as f:    \n",
    "        scores = json.load(f)\n",
    "except:\n",
    "    scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully opened the pickle file\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pred_proba = joblib.load(dataPath+'data/df_category_pred_proba.pkl')\n",
    "except:\n",
    "    pred_proba = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_proba' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d4dc03c78654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mpred_proba\u001b[0m \u001b[0;34m<>\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'data/train_log_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d_%b_%Y_%H_%M_%S\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_proba' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "if pred_proba <> {}:\n",
    "    target = open(dataPath+'data/train_log_'+strftime(\"%d_%b_%Y_%H_%M_%S\", gmtime())+'.txt', 'w')\n",
    "else:\n",
    "    target = open(dataPath+'data/train_log_'+strftime(\"%d_%b_%Y_%H_%M_%S\", gmtime())+'.txt', 'w')\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "for key in category_positive_samples:\n",
    "    if str(key) in scores:\n",
    "        continue\n",
    "    if key in scores:\n",
    "        continue\n",
    "    else:\n",
    "        find_name = df_catergory_lookup.category_id == key\n",
    "        name = df_catergory_lookup[find_name].category_name\n",
    "        the_index = name.index[0]\n",
    "\n",
    "        print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "        target.write(\"processing {0}: {1}\\n\".format(key, name.ix[the_index]))\n",
    "        category_positive_samples[key].loc[:,('is_category')] = 1\n",
    "        category_negative_samples[key].loc[:,('is_category')] = 0\n",
    "        \n",
    "        # Split into training and testing set\n",
    "        data = category_positive_samples[key].append(category_negative_samples[key], ignore_index=True)\n",
    "        X = data['item_labels']\n",
    "        y = data['is_category']\n",
    "        target.write('{:5.3f}% positive samples\\n'.format(len(category_positive_samples[key])/len(X)))\n",
    "        target.write(\"Number of datapoints in set {}\\n\".format(len(X)))\n",
    "        target.write('Training the model....\\n')\n",
    "        \n",
    "        scores[key] = cross_val_score(Ada_w2v,  X, y, cv=5, scoring='f1').mean()\n",
    "        Ada_w2v.fit(X, y)\n",
    "        pred_proba[key] = Ada_w2v.predict_proba(X)\n",
    "        \n",
    "        # persist model\n",
    "        joblib.dump(Ada_w2v, 'data/category_classifier/catg_'+str(key)+'.pkl')\n",
    "        print('Finished training category{0}: {1}\\n'.format(key, name.ix[the_index]))\n",
    "        target.write('Finished training category{0}: {1}\\n\\n'.format(key, name.ix[the_index]))\n",
    "        \n",
    "target.close()    \n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "joblib.dump(scores, dataPath+'data/df_category_scores.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get *k* vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in category_positive_samples:\n",
    "    df_ = category_positive_samples[key].append(category_negative_samples[key])\n",
    "    scores_ = pd.DataFrame(pred_proba[key], columns = ['Neg','catg_'+str(key)])\n",
    "    scores_.drop(['Neg'], axis = 1, inplace = True)\n",
    "    scores_ = scores_.set_index(df_.index)\n",
    "    df.ix[df_.index, 'catg_'+str(key)] = scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write vectors to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(dataPath+'data/df_data_vectors.dat')\n",
    "joblib.dump(pred_proba,dataPath+'data/df_category_pred_proba.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Architecture\n",
    "<figure>\n",
    "  <img src=\"images/hc_4.png\" style=\"width: 450px;\">\n",
    "</figure>\n",
    "After training the *k=141* classifiers, I extracted the *k* vectors; I carefully combined them with the engineered features, and the other raw metadata taking care to ensure that I assigned the right probabilities to the right data points in my training set. I feed these features and labels into a multi-label, multi-class AdaBoost classifier. \n",
    "\n",
    "I feed these features and labels into a multi-label, multi-class Random Forest classifier. I took 75% of the data for training and 25% for testing. Once again I do a 5 fold cross-validation scheme, fit the final classifier, and retrieve the predicted class probabilities.\n",
    "\n",
    "\n",
    "#### Get Data for Final Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "the_indices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in category_positive_samples:\n",
    "    key_df = df['label'] == key\n",
    "    the_indices[key] = df[key_df].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in the_indices:\n",
    "    X = X.append(pd.DataFrame(df.loc[the_indices[key]], columns = df.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.drop([u'mapped_category', u'category',\n",
    "u'item_name',         u'level_0',\n",
    "u'level_1',         u'level_2',         u'level_3',\n",
    "u'level_4',         u'level_5',         u'level_6',\n",
    "u'mapped_level_0',  u'mapped_level_1',  u'mapped_level_2',\n",
    "u'mapped_level_3',  u'mapped_level_4',  u'mapped_level_5',\n",
    "u'mapped_level_6',   u'item_labels'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>price_stddev</th>\n",
       "      <th>primary_unit</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>branch_lenght</th>\n",
       "      <th>item_name_match</th>\n",
       "      <th>catg_256</th>\n",
       "      <th>catg_1</th>\n",
       "      <th>catg_2</th>\n",
       "      <th>...</th>\n",
       "      <th>catg_986</th>\n",
       "      <th>catg_989</th>\n",
       "      <th>catg_998</th>\n",
       "      <th>catg_999</th>\n",
       "      <th>catg_1002</th>\n",
       "      <th>catg_495</th>\n",
       "      <th>catg_499</th>\n",
       "      <th>catg_1015</th>\n",
       "      <th>catg_1019</th>\n",
       "      <th>catg_1021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125487</th>\n",
       "      <td>602317</td>\n",
       "      <td>1.376510e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>7.99</td>\n",
       "      <td>5966</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.346282</td>\n",
       "      <td>0.565590</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232862</td>\n",
       "      <td>0.247300</td>\n",
       "      <td>0.243720</td>\n",
       "      <td>0.255702</td>\n",
       "      <td>0.191225</td>\n",
       "      <td>0.361658</td>\n",
       "      <td>0.509521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.456040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125878</th>\n",
       "      <td>609095</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4</td>\n",
       "      <td>16.81</td>\n",
       "      <td>7491</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.356767</td>\n",
       "      <td>0.557353</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283522</td>\n",
       "      <td>0.247300</td>\n",
       "      <td>0.223166</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.191225</td>\n",
       "      <td>0.334251</td>\n",
       "      <td>0.379060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125955</th>\n",
       "      <td>610507</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>18.99</td>\n",
       "      <td>5966</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.346282</td>\n",
       "      <td>0.565590</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223676</td>\n",
       "      <td>0.177823</td>\n",
       "      <td>0.243720</td>\n",
       "      <td>0.221922</td>\n",
       "      <td>0.191225</td>\n",
       "      <td>0.361658</td>\n",
       "      <td>0.471559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126064</th>\n",
       "      <td>612597</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12059</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386763</td>\n",
       "      <td>0.541789</td>\n",
       "      <td>0.377839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262988</td>\n",
       "      <td>0.284572</td>\n",
       "      <td>0.350169</td>\n",
       "      <td>0.221922</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>0.334251</td>\n",
       "      <td>0.415429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.463727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127054</th>\n",
       "      <td>512281</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18</td>\n",
       "      <td>23.50</td>\n",
       "      <td>1246</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386763</td>\n",
       "      <td>0.541789</td>\n",
       "      <td>0.377839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262988</td>\n",
       "      <td>0.284572</td>\n",
       "      <td>0.350169</td>\n",
       "      <td>0.221922</td>\n",
       "      <td>0.293972</td>\n",
       "      <td>0.334251</td>\n",
       "      <td>0.347207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.463727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id  price_stddev  primary_unit  price_mean  vendor_id  \\\n",
       "125487   602317  1.376510e-07             3        7.99       5966   \n",
       "125878   609095  0.000000e+00             4       16.81       7491   \n",
       "125955   610507  0.000000e+00             1       18.99       5966   \n",
       "126064   612597  0.000000e+00             3        8.25      12059   \n",
       "127054   512281  0.000000e+00            18       23.50       1246   \n",
       "\n",
       "        branch_lenght  item_name_match  catg_256    catg_1    catg_2  \\\n",
       "125487              4              0.0  0.346282  0.565590  0.360200   \n",
       "125878              4              1.0  0.356767  0.557353  0.360200   \n",
       "125955              4              1.0  0.346282  0.565590  0.360200   \n",
       "126064              4              0.0  0.386763  0.541789  0.377839   \n",
       "127054              4              0.0  0.386763  0.541789  0.377839   \n",
       "\n",
       "          ...      catg_986  catg_989  catg_998  catg_999  catg_1002  \\\n",
       "125487    ...      0.232862  0.247300  0.243720  0.255702   0.191225   \n",
       "125878    ...      0.283522  0.247300  0.223166  0.007575   0.191225   \n",
       "125955    ...      0.223676  0.177823  0.243720  0.221922   0.191225   \n",
       "126064    ...      0.262988  0.284572  0.350169  0.221922   0.214968   \n",
       "127054    ...      0.262988  0.284572  0.350169  0.221922   0.293972   \n",
       "\n",
       "        catg_495  catg_499  catg_1015  catg_1019  catg_1021  \n",
       "125487  0.361658  0.509521        0.0        0.0   0.456040  \n",
       "125878  0.334251  0.379060        0.0        0.0   0.551332  \n",
       "125955  0.361658  0.471559        0.0        0.0   0.501350  \n",
       "126064  0.334251  0.415429        0.0        0.0   0.463727  \n",
       "127054  0.334251  0.347207        0.0        0.0   0.463727  \n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all rows whose label is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X[X['label'].notnull()]\n",
    "X = X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = X['label']\n",
    "X.drop(['label'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in set 32788\n",
      "Successfully shuffled and split the data!\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of datapoints in set {}\".format(len(X)))\n",
    "\n",
    "# First, decide how many training vs test samples you want\n",
    "# I am going to use a train, validate and test split set\n",
    "\n",
    "num_train = int(len(y) * .80)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "try:\n",
    "    X_train, y_train, X_test, y_test = shuffle_split_data(X, y)\n",
    "    print (\"Successfully shuffled and split the data!\")\n",
    "except:\n",
    "    print (\"Something went wrong with shuffling and splitting the data.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION RESULTS OF BASELINE CLASSIFIERS\n",
      "\n",
      "CLASSIFIER           MEAN SCORE %  STD DEV %    TIME   \n",
      "RandomForest             99.39        0.11         7.28secs\n",
      "DecisionTree             99.15        0.21        10.64secs\n",
      "SVC                     100.00        0.01        93.60secs\n",
      "AdaBoost                 24.91        4.38        15.99secs\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "          'DecisionTree': DecisionTreeClassifier(random_state=seed),\n",
    "          'SVC': LinearSVC(random_state=seed),\n",
    "          'RandomForest': RandomForestClassifier(random_state=seed),\n",
    "          'AdaBoost': AdaBoostClassifier(n_estimators=10, random_state=seed)\n",
    "         }\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_transform = scaler.fit_transform(X_train)\n",
    "\n",
    "print('CLASSIFICATION RESULTS OF BASELINE CLASSIFIERS\\n')\n",
    "print('{:20}{:^15}{:^10}{:>10}'.format('CLASSIFIER', 'MEAN SCORE %', 'STD DEV %', 'TIME'))\n",
    "\n",
    "\n",
    "for clf_name, clf in models.iteritems():\n",
    "    t0 = time()\n",
    "    results = cross_val_score(clf, X_transform, y_train, cv=5)\n",
    "    t1 = time() - t0\n",
    "    print('{:20}{:^15.2f}{:^10.2f}{:>10.2f}secs'.format(clf_name, results.mean()*100, results.std()*100, t1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:          96.943\n",
      "Recall:   95.149  \n",
      "F Score  95.761  \n",
      "Support: 118.000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = RandomForestClassifier(random_state=seed)\n",
    "clf.fit(X_transform, y_train)\n",
    "X_test_X_transform = scaler.transform(X_test)\n",
    "final_preds = clf.predict(X_test_X_transform)\n",
    "precision, recall, fbeta_score, support = score(y_test, final_preds)\n",
    "\n",
    "\n",
    "print (\"Precision: {:10.3f}\\nRecall: {:^10.3f}\\nF Score{:^10.3f}\\nSupport:{:^10.3f}\".format(precision.mean()*100, \n",
    "                                                 recall.mean()*100, fbeta_score.mean()*100, len(support)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_test, final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Result\n",
    "With the understanding that chance is 1/118 * 100 = 0.8%, my approach achieves:    \n",
    "\n",
    "\n",
    "|Metric|Value|\n",
    "|---|---|\n",
    "|Precision:| 96.943  |\n",
    "|Recall:| 95.149 |\n",
    "|F Score: | 95.761  |\n",
    "|Classes:  |  118  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in category_positive_samples:\n",
    "        find_name = df_catergory_lookup.category_id == key\n",
    "        name = df_catergory_lookup[find_name].category_name\n",
    "        the_index = name.index[0]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outX = pd.DataFrame(index=y_test.index)\n",
    "outX = outX.join(y_test)\n",
    "outX['predicted'] = final_preds\n",
    "outX['predicted_name'] = ''\n",
    "outX['item_name'] = pd.DataFrame(df.loc[outX.index])['item_name']\n",
    "outX['mapped_level_0'] = pd.DataFrame(df.loc[outX.index])['mapped_level_0']\n",
    "outX['level_0'] = pd.DataFrame(df.loc[outX.index])['level_0']\n",
    "outX['mapped_level_1'] = pd.DataFrame(df.loc[outX.index])['mapped_level_1']\n",
    "outX['level_1'] = pd.DataFrame(df.loc[outX.index])['level_1']\n",
    "outX['mapped_level_2'] = pd.DataFrame(df.loc[outX.index])['mapped_level_2']\n",
    "outX['level_2'] = pd.DataFrame(df.loc[outX.index])['level_2']\n",
    "outX['mapped_level_3'] = pd.DataFrame(df.loc[outX.index])['mapped_level_3']\n",
    "outX['level_3'] = pd.DataFrame(df.loc[outX.index])['level_3']\n",
    "outX['mapped_level_4'] = pd.DataFrame(df.loc[outX.index])['mapped_level_4']\n",
    "outX['level_4'] = pd.DataFrame(df.loc[outX.index])['level_4']\n",
    "outX['mapped_level_5'] = pd.DataFrame(df.loc[outX.index])['mapped_level_5']\n",
    "outX['level_5'] = pd.DataFrame(df.loc[outX.index])['level_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in final_preds:\n",
    "    find_name = df_catergory_lookup.category_id == i\n",
    "    outX.ix[outX.index[j], 'predicted_name'] = df_catergory_lookup[find_name]['category_name'].values[0]\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#score(y_test, final_preds)\n",
    "#outX.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a category find ancestors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain classifier for category k = 235, chocolate\n",
    "The value of this method is that I can take a look at the results, determine which individual text-based classifiers have poor results, and optimize and tune that specific classifier. In the case of the chocolate classifier, I see that the minority class is minuscule. To deal with this great imbalance in the data, I synthetically resample it and retrain the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "key = 235\n",
    "\n",
    "data = category_positive_samples[key].append(category_negative_samples[key], ignore_index=True)\n",
    "X_235 = data['item_labels']\n",
    "y_235 = data['is_category']\n",
    "        \n",
    "vectorizer = MeanEmbeddingVectorizer(w2v)\n",
    "vectorizer.fit(X_235, y_235)  \n",
    "\n",
    "sm = SMOTE(random_state=seed)\n",
    "\n",
    "print('Original dataset shape {}'.format(Counter(y_235)))\n",
    "X_res, y_res = sm.fit_sample(vectorizer.transform(X_235) , y_235)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Training the model....')\n",
    "clf = AdaBoostClassifier(n_estimators=10)\n",
    "scores_235 = cross_val_score(clf, X_res, y_res, cv=5, scoring='f1').mean()\n",
    "clf.fit(X_res, y_res)\n",
    "pred_proba_235 = clf.predict_proba(X_res)\n",
    "print('Finished training category\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = vectorizer.transform(X_235)\n",
    "clf.fit(X_, y_235)\n",
    "pred_proba_235 = clf.predict_proba(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores[key] = scores_235\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key = 235\n",
    "df_ = category_positive_samples[key].append(category_negative_samples[key])\n",
    "scores_ = pd.DataFrame(pred_proba_235, columns = ['Neg','catg_'+str(key)])\n",
    "scores_.drop(['Neg'], axis = 1, inplace = True)\n",
    "scores_ = scores_.set_index(df_.index)\n",
    "df.ix[df_.index, 'catg_'+str(key)] = scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
