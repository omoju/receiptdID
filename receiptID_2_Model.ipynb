{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)\n",
    "\n",
    "# This line will hide code by default when the notebook is exported as HTML\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n",
    "\n",
    "# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\n",
    "di.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>''', raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receipt.ID\n",
    "### Hierarchical text item classification\n",
    "Taxonomic classification, categorize items according to a pre-defined taxonomy. The goal is to assign one or more categories in the taxonomy to an item. It is a multi-class **and** multi-label classification problem with hierarchical relationships between each node in the tree.\n",
    "\n",
    "#### Items\n",
    "- Items come from a wide range for categories like Produce, Meat, Beverage, Supplies. \n",
    "- Example item to category mapping:\n",
    "\n",
    "\n",
    "|item|mapping|\n",
    "|---|---|\n",
    "|Kale  | \"Food/Produce/Kale\"  |\n",
    "|Vinegar white wine 50 grain  | \"Food/Dry-Grocery/Vinegars/White Wine Vinegar\"  |\n",
    "|Imported nat flank steak  | \"Food/Meats/Beef/Flank Steak\"  |\n",
    "\n",
    "\n",
    "To solve this problem, I will undertake the following course of action:\n",
    "1. Explore the dataset\n",
    "    - Explore the dataset to ensure its integrity and understand the context. \n",
    "2. Identify features that may be used. \n",
    "    - If possible, engineer features that might provide greater discrimination.\n",
    "3. Build k independent *text-based* classifiers for the text-based features and feed the output from these classifiers into the next layer classifier which takes in the other features. Explore a couple of classifiers that might be well suited for the problem at hand.\n",
    "    - Decision Trees\n",
    "    - SVM\n",
    "    - AdaBoost\n",
    "4.  Select appropriate classifier based on evaluation metric and tune it for optimality.\n",
    "\n",
    "In this notebook I do processes 3 and 4.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Graphing Libraries\n",
    "import matplotlib.pyplot as pyplt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_pickle('data/df_data_vectors.dat')\n",
    "df_catergory_lookup = pd.read_pickle('data/data_category_lookup.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127108"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def shuffle_split_data(X, y):\n",
    "    \"\"\" Shuffles and splits data into 75% training and 25% testing subsets,\n",
    "        then returns the training and testing subsets. \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=num_train, random_state=42)\n",
    "    \n",
    "    # Return the training and testing data subsets\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_categories(level, num_data = 1000):\n",
    "    \n",
    "    the_categories = {}\n",
    "    counts = df.groupby(level).size()\n",
    "    total = df.groupby(level).size().sum()\n",
    "    \n",
    "    for i in counts.index:\n",
    "        if counts[i] > num_data:\n",
    "            the_categories[int(i)] = counts[i]\n",
    "\n",
    "\n",
    "    print(len(the_categories), 'categories with enough data\\n')  \n",
    "\n",
    "    for key in the_categories:\n",
    "        find_name = df_catergory_lookup.category_id == key\n",
    "        name = df_catergory_lookup[find_name].category_name\n",
    "        the_index = name.index[0]\n",
    "\n",
    "        #print (\"{0}: {1}\".format(key, name.ix[the_index]))\n",
    "        \n",
    "    return the_categories, total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current approach\n",
    "The current approach used focused exclusively on the item's name, for example, a data point in the dataset would have an object's name as \"Strauss Yogurt\" or \"Organic spinach.\" The first challenge with this approach lay in the fact that the item label was quite short, roughly about four to eight words.  Further, when modifiers like *gluten free*, *organic*, or *pesticide free* wherein the item's label, this added a layer of misinformation causing items like *organic milk* and *organic beer* to be classified in the same class.\n",
    "\n",
    "## Samples\n",
    "I have selected these samples to see that I can correctly separate them even though they have the modifiers in their item name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen samples of items:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mapped_category</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price_stddev</th>\n",
       "      <th>primary_unit</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>category</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>...</th>\n",
       "      <th>catg_986</th>\n",
       "      <th>catg_989</th>\n",
       "      <th>catg_998</th>\n",
       "      <th>catg_999</th>\n",
       "      <th>catg_1002</th>\n",
       "      <th>catg_495</th>\n",
       "      <th>catg_499</th>\n",
       "      <th>catg_1015</th>\n",
       "      <th>catg_1019</th>\n",
       "      <th>catg_1021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>[Food, Produce, Spinach]</td>\n",
       "      <td>1616</td>\n",
       "      <td>0.539695</td>\n",
       "      <td>1</td>\n",
       "      <td>12.6064</td>\n",
       "      <td>[1, 9, 91]</td>\n",
       "      <td>14</td>\n",
       "      <td>organic baby spinach</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011264</td>\n",
       "      <td>0.184686</td>\n",
       "      <td>0.235981</td>\n",
       "      <td>0.247730</td>\n",
       "      <td>0.230250</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>0.240510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64391</th>\n",
       "      <td>[Beverages, Alcoholic, Beers]</td>\n",
       "      <td>508958</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>165</td>\n",
       "      <td>[3, 6, 16]</td>\n",
       "      <td>248</td>\n",
       "      <td>eel river organic 15.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.267325</td>\n",
       "      <td>0.247167</td>\n",
       "      <td>0.01175</td>\n",
       "      <td>0.252006</td>\n",
       "      <td>0.307585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55697</th>\n",
       "      <td>[Food, Meats, Beef, Beef Tongue]</td>\n",
       "      <td>455407</td>\n",
       "      <td>0.0151186</td>\n",
       "      <td>3</td>\n",
       "      <td>2.98429</td>\n",
       "      <td>[1, 7, 70, 1675]</td>\n",
       "      <td>4218</td>\n",
       "      <td>mary's organic fryers</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265593</td>\n",
       "      <td>0.219891</td>\n",
       "      <td>0.352890</td>\n",
       "      <td>0.281422</td>\n",
       "      <td>0.247474</td>\n",
       "      <td>0.361146</td>\n",
       "      <td>0.338465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mapped_category item_id price_stddev primary_unit  \\\n",
       "274            [Food, Produce, Spinach]    1616     0.539695            1   \n",
       "64391     [Beverages, Alcoholic, Beers]  508958         None            4   \n",
       "55697  [Food, Meats, Beef, Beef Tongue]  455407    0.0151186            3   \n",
       "\n",
       "      price_mean          category vendor_id               item_name  level_0  \\\n",
       "274      12.6064        [1, 9, 91]        14    organic baby spinach        1   \n",
       "64391        165        [3, 6, 16]       248  eel river organic 15.5        3   \n",
       "55697    2.98429  [1, 7, 70, 1675]      4218   mary's organic fryers        1   \n",
       "\n",
       "       level_1    ...     catg_986  catg_989  catg_998  catg_999  catg_1002  \\\n",
       "274        9.0    ...     0.011264  0.184686  0.235981  0.247730   0.230250   \n",
       "64391      6.0    ...     0.316517       NaN       NaN       NaN        NaN   \n",
       "55697      7.0    ...     0.265593  0.219891  0.352890  0.281422   0.247474   \n",
       "\n",
       "       catg_495  catg_499 catg_1015 catg_1019 catg_1021  \n",
       "274    0.290698  0.240510       NaN       NaN       NaN  \n",
       "64391  0.267325  0.247167   0.01175  0.252006  0.307585  \n",
       "55697  0.361146  0.338465       NaN       NaN       NaN  \n",
       "\n",
       "[3 rows x 164 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices = [274, 64391, 55697] \n",
    "\n",
    "# Create a DataFrame of the chosen samples\n",
    "samples = pd.DataFrame(df.loc[indices], columns = df.keys())\n",
    "print (\"Chosen samples of items:\")\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## My Approach\n",
    "I took an entirely different approach. I got inspiration from the approach that Google, [YouTube](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36411.pdf) used in organizing videos and decided to shift the unit of analysis from item *name* to the *categories* themselves. My approach combines information from both the text-based labels as well as the item's metadata.\n",
    "\n",
    "This method achieves two crucial things. First, by focusing on individual categories, each time a new category of item is added to the restaurant domain, instead of having to retrain the classifier on the entire dataset, all we have to do is gather enough data for that category, and train a classifier for it. This way, the approach can scale beautifully as the taxonomy grows. Second, moving the unit of analysis from text labels to categories, it becomes easier to correctly separate \"organic cream\" and \"organic beer.\" \n",
    "\n",
    "I chose to discard categories that had less than 300 datapoints. As the datapoints in the category grows, those categories can then been trained individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 categories with enough data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "the_level = {}\n",
    "num_data = 300\n",
    "\n",
    "level = 'level_0'\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 categories with enough data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "level = 'level_1'\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 categories with enough data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "level = 'level_2'\n",
    "df.loc[:,('label')] = df.loc[:, level]\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 categories with enough data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "level = 'level_3'\n",
    "df.loc[:,('label')] = df.loc[:, level]\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 categories with enough data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "level = 'level_4'\n",
    "df.loc[:,('label')] = df.loc[:, level]\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 141 categories in all\n"
     ]
    }
   ],
   "source": [
    "k_categories = 0\n",
    "for key in the_level.keys():\n",
    "    k_categories += len(the_level[key].keys())\n",
    "   \n",
    "    \n",
    "print('processed {} categories in all'.format(k_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling\n",
    "For each data point, I assigned the deepest class node in the tree to which it belonged as its label. For example, an item named \"bacon ends\" belonged to classes [Food, Meats, Pork]. For such an item, I assigned it is label as \"Pork.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_examples_ = df['label'].isnull()\n",
    "df.ix[df[null_examples_].index, 'label'] = df.loc[:, 'level_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mapped_category</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price_stddev</th>\n",
       "      <th>primary_unit</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>category</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>...</th>\n",
       "      <th>catg_986</th>\n",
       "      <th>catg_989</th>\n",
       "      <th>catg_998</th>\n",
       "      <th>catg_999</th>\n",
       "      <th>catg_1002</th>\n",
       "      <th>catg_495</th>\n",
       "      <th>catg_499</th>\n",
       "      <th>catg_1015</th>\n",
       "      <th>catg_1019</th>\n",
       "      <th>catg_1021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mapped_category, item_id, price_stddev, primary_unit, price_mean, category, vendor_id, item_name, level_0, level_1, level_2, level_3, level_4, level_5, level_6, mapped_level_0, mapped_level_1, mapped_level_2, mapped_level_3, mapped_level_4, mapped_level_5, mapped_level_6, branch_lenght, item_name_match, item_labels, label, catg_256, catg_1, catg_2, catg_3, catg_4, catg_5, catg_6, catg_7, catg_8, catg_9, catg_10, catg_12, catg_269, catg_1039, catg_16, catg_18, catg_19, catg_537, catg_535, catg_260, catg_26, catg_134, catg_177, catg_263, catg_392, catg_51, catg_1077, catg_54, catg_55, catg_56, catg_1082, catg_322, catg_1078, catg_70, catg_456, catg_329, catg_74, catg_76, catg_78, catg_464, catg_470, catg_344, catg_482, catg_111, catg_1030, catg_1033, catg_1041, catg_531, catg_21, catg_32, catg_1200, catg_35, catg_39, catg_40, catg_44, catg_49, catg_50, catg_52, catg_69, catg_80, catg_84, catg_86, catg_87, catg_88, catg_89, catg_95, catg_98, catg_99, catg_105, catg_109, catg_115, catg_118, catg_119, catg_124, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 164 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the *k* training sets for the *k* categories for each level in the tree   \n",
    "The aim of moving to a category-based solution is to embed knowledge of the taxonomy into classifiers. To do this, I had to figure out how to get positive and negative samples for each category. For every category node, I decided that itself, as well as all its descendants, were *positive* samples for that class. All other nodes that were neither the categories ancestor(s) or itself were set as *negative* samples.  The figure below gives a visual explanation of selecting category training set. I did this for each category node in the tree that had enough data.\n",
    "<img src=\"images/hc_5.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "One of the limitations of this method is that most nodes have un-balanced classes. Some more severe than other, especially as you go further down in the tree. There are some classes whose ratio of positive signals is as small as 0.02%. The more granular that subclasses get, the harder it is to classify them. The good news is that one can decide to focus on a few of these classes and use synthetic methods to rebalance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dict of positive and negative samples for the categories\n",
    "category_positive_samples = {}\n",
    "category_negative_samples= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1: Food\n",
      "processing 2: Supplies\n",
      "processing 3: Beverages\n",
      "processing 4: Other\n",
      "processing 495: Grocery\n",
      "processing 499: Protein\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_0'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "    \n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_1'].isnull()\n",
    "    positive_examples = df['level_1'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_positive_samples[key] = df[key_df & null_examples]\n",
    "        category_positive_samples[key] = category_positive_samples[key].append(df[key_df & positive_examples])\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    level_0_item = df['level_0'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1281: SF Checkout Bag Fee\n",
      "processing 130: Cream\n",
      "processing 375: Sausages\n",
      "processing 132: Tomatoes\n",
      "processing 134: Peppers\n",
      "processing 391: Salt\n",
      "processing 392: Disposables & Packaging Supplies\n",
      "processing 267: Glasses\n",
      "processing 258: Keg Deposit\n",
      "processing 143: Tortillas\n",
      "processing 16: Beers\n",
      "processing 145: Cucumbers\n",
      "processing 18: Wines\n",
      "processing 531: Condiments\n",
      "processing 535: Fruits\n",
      "processing 152: Flour & Starch\n",
      "processing 537: Herbs\n",
      "processing 26: Juices\n",
      "processing 157: Garlic\n",
      "processing 1030: Seeds\n",
      "processing 40: Teas\n",
      "processing 44: Beans\n",
      "processing 482: Linens\n",
      "processing 177: Onions\n",
      "processing 51: Fuel & Freight/Delivery\n",
      "processing 1077: Breads\n",
      "processing 54: Liquor\n",
      "processing 55: Oils\n",
      "processing 56: Pork\n",
      "processing 1730: Leaves\n",
      "processing 325: Eggs\n",
      "processing 70: Beef\n",
      "processing 456: Sodas\n",
      "processing 329: Poultry\n",
      "processing 74: Fish\n",
      "processing 331: Coffee\n",
      "processing 76: Spices\n",
      "processing 226: Vinegars\n",
      "processing 78: Cheese\n",
      "processing 464: Canned\n",
      "processing 1272: Squash\n",
      "processing 84: Lettuce\n",
      "processing 1365: Spreads & Pastes\n",
      "processing 86: Asparagus\n",
      "processing 87: Syrup\n",
      "processing 216: Radishes\n",
      "processing 89: Kale\n",
      "processing 986: Cleaning & Janitorial\n",
      "processing 1039: Nuts & Grains\n",
      "processing 98: Beets\n",
      "processing 228: Sugar\n",
      "processing 1041: Medical Supplies\n",
      "processing 105: Lamb\n",
      "processing 235: Chocolate\n",
      "processing 109: Broccoli\n",
      "processing 111: Mushrooms\n",
      "processing 1960: Glass Cleaner\n",
      "processing 115: Butter \n",
      "processing 118: Potatoes\n",
      "processing 119: Cabbages\n",
      "processing 332: Pasta\n",
      "processing 1078: Shellfish\n",
      "processing 124: Milk\n",
      "processing 125: Carrots\n",
      "processing 126: Cauliflower\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_2'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_3'].isnull()\n",
    "    positive_examples = df['level_3'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_positive_samples[key] = df[key_df & null_examples]\n",
    "        category_positive_samples[key] = category_positive_samples[key].append(df[key_df & positive_examples])    \n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    find_level_0 = df[level] == key\n",
    "    name_level_0 = df[find_level_0].level_0\n",
    "    the_index_level_0 = name_level_0.index[0]\n",
    "    \n",
    "    find_level_1 = df[level] == key\n",
    "    name_level_1 = df[find_level_1].level_1\n",
    "    the_index_level_1 = name_level_1.index[0]\n",
    "    \n",
    "    level_0_item = df['level_0'] != name_level_0.ix[the_index_level_0]\n",
    "    level_1_item = df['level_1'] != name_level_1.ix[the_index_level_1]\n",
    "    level_2_item = df['level_2'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item & level_1_item & level_2_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 387: Oysters\n",
      "processing 136: Cilantro\n",
      "processing 1033: Ground Black Pepper\n",
      "processing 1802: Baby Carrot\n",
      "processing 1239: Lids\n",
      "processing 275: Containers\n",
      "processing 148: Olives\n",
      "processing 25: Vodka\n",
      "processing 282: Uniforms\n",
      "processing 286: Bags\n",
      "processing 32: Gin\n",
      "processing 289: Cups\n",
      "processing 164: Mints\n",
      "processing 39: Tequila\n",
      "processing 176: Oranges\n",
      "processing 1160: Red Wines\n",
      "processing 50: Iced Tea\n",
      "processing 52: Whiskey\n",
      "processing 437: Bacon\n",
      "processing 1209: Gloves\n",
      "processing 319: Red Onions\n",
      "processing 963: Mozzarella\n",
      "processing 69: Chicken\n",
      "processing 970: Extra Virgin Olive Oil\n",
      "processing 1200: Shrimp\n",
      "processing 1358: Rice\n",
      "processing 80: Apples\n",
      "processing 355: Duck\n",
      "processing 980: Dried Fruits\n",
      "processing 983: Cheddar\n",
      "processing 88: Avocados\n",
      "processing 1113: Kegs\n",
      "processing 95: Basil\n",
      "processing 99: Bell Peppers\n",
      "processing 229: Sauces\n",
      "processing 998: White Wines\n",
      "processing 363: Rum\n",
      "processing 403: Purees\n",
      "processing 247: Zucchini Squash\n",
      "processing 1019: Salmon\n",
      "processing 1148: Liqueur\n",
      "processing 1021: Tuna\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_3'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_4'].isnull()\n",
    "    positive_examples = df['level_4'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_positive_samples[key] = df[key_df & null_examples]\n",
    "        category_positive_samples[key] = category_positive_samples[key].append(df[key_df & positive_examples])   \n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    find_level_0 = df[level] == key\n",
    "    name_level_0 = df[find_level_0].level_0\n",
    "    the_index_level_0 = name_level_0.index[0]\n",
    "    \n",
    "    find_level_1 = df[level] == key\n",
    "    name_level_1 = df[find_level_1].level_1\n",
    "    the_index_level_1 = name_level_1.index[0]\n",
    "    \n",
    "    find_level_2 = df[level] == key\n",
    "    name_level_2 = df[find_level_2].level_1\n",
    "    the_index_level_2 = name_level_2.index[0]\n",
    "    \n",
    "    level_0_item = df['level_0'] != name_level_0.ix[the_index_level_0]\n",
    "    level_1_item = df['level_1'] != name_level_1.ix[the_index_level_1]\n",
    "    level_2_item = df['level_2'] != name_level_2.ix[the_index_level_2]\n",
    "    level_3_item = df['level_3'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item & level_1_item & level_2_item & level_3_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 35: Bourbon\n",
      "processing 999: Cabernet Sauvignon\n",
      "processing 1064: Chardonnay\n",
      "processing 1002: Sauvignon Blanc\n",
      "processing 1164: Brut\n",
      "processing 49: Rye\n",
      "processing 21: Rose Wine\n",
      "processing 1015: Chicken Breast\n",
      "processing 989: Pinot Noir\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_4'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_5'].isnull()\n",
    "    positive_examples = df['level_5'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_positive_samples[key] = df[key_df & null_examples]\n",
    "        category_positive_samples[key] = category_positive_samples[key].append(df[key_df & positive_examples])\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    find_level_0 = df[level] == key\n",
    "    name_level_0 = df[find_level_0].level_0\n",
    "    the_index_level_0 = name_level_0.index[0]\n",
    "    \n",
    "    find_level_1 = df[level] == key\n",
    "    name_level_1 = df[find_level_1].level_1\n",
    "    the_index_level_1 = name_level_1.index[0]\n",
    "    \n",
    "    find_level_2 = df[level] == key\n",
    "    name_level_2 = df[find_level_2].level_1\n",
    "    the_index_level_2 = name_level_2.index[0]\n",
    "    \n",
    "    find_level_3 = df[level] == key\n",
    "    name_level_3 = df[find_level_3].level_1\n",
    "    the_index_level_3 = name_level_3.index[0]\n",
    "    \n",
    "    level_0_item = df['level_0'] != name_level_0.ix[the_index_level_0]\n",
    "    level_1_item = df['level_1'] != name_level_1.ix[the_index_level_1]\n",
    "    level_2_item = df['level_2'] != name_level_2.ix[the_index_level_2]\n",
    "    level_3_item = df['level_3'] != name_level_3.ix[the_index_level_3]\n",
    "    level_4_item = df['level_4'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item & level_1_item & level_2_item & level_3_item & level_4_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_positive_samples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mapped_category</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price_stddev</th>\n",
       "      <th>primary_unit</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>category</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>...</th>\n",
       "      <th>catg_986</th>\n",
       "      <th>catg_989</th>\n",
       "      <th>catg_998</th>\n",
       "      <th>catg_999</th>\n",
       "      <th>catg_1002</th>\n",
       "      <th>catg_495</th>\n",
       "      <th>catg_499</th>\n",
       "      <th>catg_1015</th>\n",
       "      <th>catg_1019</th>\n",
       "      <th>catg_1021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125929</th>\n",
       "      <td>[Beverages, Alcoholic, Liquor, Whiskey, Bourbo...</td>\n",
       "      <td>610087</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>182.4</td>\n",
       "      <td>[3, 6, 54, 52, 35, 2439]</td>\n",
       "      <td>9396</td>\n",
       "      <td>jim beam rye 90</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.259861</td>\n",
       "      <td>0.283328</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.150631</td>\n",
       "      <td>0.175491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126081</th>\n",
       "      <td>[Beverages, Alcoholic, Liquor, Whiskey, Bourbo...</td>\n",
       "      <td>612831</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>35.95</td>\n",
       "      <td>[3, 6, 54, 52, 35, 2426]</td>\n",
       "      <td>9530</td>\n",
       "      <td>basil haydens bourbon 80</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.310331</td>\n",
       "      <td>0.283328</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.181986</td>\n",
       "      <td>0.348535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126665</th>\n",
       "      <td>[Beverages, Alcoholic, Liquor, Whiskey, Bourbo...</td>\n",
       "      <td>528004</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>45.8</td>\n",
       "      <td>[3, 6, 54, 52, 35, 2481]</td>\n",
       "      <td>10335</td>\n",
       "      <td>breaker bbn port finish 90</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.281260</td>\n",
       "      <td>0.247167</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.214430</td>\n",
       "      <td>0.353754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          mapped_category item_id  \\\n",
       "125929  [Beverages, Alcoholic, Liquor, Whiskey, Bourbo...  610087   \n",
       "126081  [Beverages, Alcoholic, Liquor, Whiskey, Bourbo...  612831   \n",
       "126665  [Beverages, Alcoholic, Liquor, Whiskey, Bourbo...  528004   \n",
       "\n",
       "       price_stddev primary_unit price_mean                  category  \\\n",
       "125929         None            1      182.4  [3, 6, 54, 52, 35, 2439]   \n",
       "126081         None           21      35.95  [3, 6, 54, 52, 35, 2426]   \n",
       "126665         None           21       45.8  [3, 6, 54, 52, 35, 2481]   \n",
       "\n",
       "       vendor_id                   item_name  level_0  level_1    ...     \\\n",
       "125929      9396             jim beam rye 90        3      6.0    ...      \n",
       "126081      9530    basil haydens bourbon 80        3      6.0    ...      \n",
       "126665     10335  breaker bbn port finish 90        3      6.0    ...      \n",
       "\n",
       "        catg_986  catg_989  catg_998  catg_999  catg_1002  catg_495  catg_499  \\\n",
       "125929  0.258597       NaN       NaN       NaN        NaN  0.259861  0.283328   \n",
       "126081  0.324394       NaN       NaN       NaN        NaN  0.310331  0.283328   \n",
       "126665  0.355630       NaN       NaN       NaN        NaN  0.281260  0.247167   \n",
       "\n",
       "       catg_1015 catg_1019 catg_1021  \n",
       "125929  0.000254  0.150631  0.175491  \n",
       "126081  0.000254  0.181986  0.348535  \n",
       "126665  0.011750  0.214430  0.353754  \n",
       "\n",
       "[3 rows x 164 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_positive_samples[35].tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mapped_category</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price_stddev</th>\n",
       "      <th>primary_unit</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>category</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>...</th>\n",
       "      <th>catg_986</th>\n",
       "      <th>catg_989</th>\n",
       "      <th>catg_998</th>\n",
       "      <th>catg_999</th>\n",
       "      <th>catg_1002</th>\n",
       "      <th>catg_495</th>\n",
       "      <th>catg_499</th>\n",
       "      <th>catg_1015</th>\n",
       "      <th>catg_1019</th>\n",
       "      <th>catg_1021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Supplies, Restaurant Supplies, Disposables &amp; ...</td>\n",
       "      <td>167</td>\n",
       "      <td>7.55423e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>28.7</td>\n",
       "      <td>[2, 12, 392, 430]</td>\n",
       "      <td>5</td>\n",
       "      <td>fork 10in serving black 144/cs</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264482</td>\n",
       "      <td>0.288393</td>\n",
       "      <td>0.363877</td>\n",
       "      <td>0.251602</td>\n",
       "      <td>0.230086</td>\n",
       "      <td>0.264524</td>\n",
       "      <td>0.180126</td>\n",
       "      <td>0.00914</td>\n",
       "      <td>0.166362</td>\n",
       "      <td>0.306131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Food, Meats, Pork]</td>\n",
       "      <td>176</td>\n",
       "      <td>6.06195e-08</td>\n",
       "      <td>3</td>\n",
       "      <td>2.19</td>\n",
       "      <td>[1, 7, 56]</td>\n",
       "      <td>7</td>\n",
       "      <td>bacon ends 15# (kruse)</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202777</td>\n",
       "      <td>0.197688</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.214336</td>\n",
       "      <td>0.183216</td>\n",
       "      <td>0.285991</td>\n",
       "      <td>0.331882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Food, Meats, Beef]</td>\n",
       "      <td>178</td>\n",
       "      <td>4.11051e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>8.45</td>\n",
       "      <td>[1, 7, 70]</td>\n",
       "      <td>7</td>\n",
       "      <td>flat iron steak ready choice</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355630</td>\n",
       "      <td>0.286951</td>\n",
       "      <td>0.285717</td>\n",
       "      <td>0.280367</td>\n",
       "      <td>0.197030</td>\n",
       "      <td>0.288223</td>\n",
       "      <td>0.306360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     mapped_category item_id price_stddev  \\\n",
       "0  [Supplies, Restaurant Supplies, Disposables & ...     167  7.55423e-07   \n",
       "1                                [Food, Meats, Pork]     176  6.06195e-08   \n",
       "2                                [Food, Meats, Beef]     178  4.11051e-07   \n",
       "\n",
       "  primary_unit price_mean           category vendor_id  \\\n",
       "0            1       28.7  [2, 12, 392, 430]         5   \n",
       "1            3       2.19         [1, 7, 56]         7   \n",
       "2            3       8.45         [1, 7, 70]         7   \n",
       "\n",
       "                        item_name  level_0  level_1    ...     catg_986  \\\n",
       "0  fork 10in serving black 144/cs        2     12.0    ...     0.264482   \n",
       "1          bacon ends 15# (kruse)        1      7.0    ...     0.202777   \n",
       "2    flat iron steak ready choice        1      7.0    ...     0.355630   \n",
       "\n",
       "   catg_989  catg_998  catg_999  catg_1002  catg_495  catg_499 catg_1015  \\\n",
       "0  0.288393  0.363877  0.251602   0.230086  0.264524  0.180126   0.00914   \n",
       "1  0.197688  0.264600  0.214336   0.183216  0.285991  0.331882       NaN   \n",
       "2  0.286951  0.285717  0.280367   0.197030  0.288223  0.306360       NaN   \n",
       "\n",
       "  catg_1019 catg_1021  \n",
       "0  0.166362  0.306131  \n",
       "1       NaN       NaN  \n",
       "2       NaN       NaN  \n",
       "\n",
       "[3 rows x 164 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_negative_samples[35].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "With the training sets done, I vectorized them using a mean embedding vectorizer. For each category, I created an AdaBoost binary classifier. I chose to use the AdaBoost after I experimented with several classifiers like LinearSVM and Decision Trees. I first selected Decision Tree since it outperformed the LinearSVM for this problem. However, I quickly realized my aim was to get predicted probabilities as output for the class instead of simple binary output which the Decision Tree gave. For that reason, I switched to AdaBoost, which is a *boosted* decision tree. \n",
    "\n",
    "I train each classifier using a k=5 kfold cross-validation scheme. I fitted the resulting classifier and retrieved the predicted probability for each data point in the dataset, which resulted in *k* vectors for *k* categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process item label names using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Only use item labels as input into word2vec embeddings\n",
    "# train word2vec on all the item_labels \n",
    "\n",
    "w2v_model = Word2Vec(df['item_labels'], size=750, window=5, min_count=5, workers=4)\n",
    "w2v = dict(zip(w2v_model.index2word, w2v_model.syn0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True) #to trim unneeded model memory = use (much) less RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train *k* classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seed = 342 # For reproducability\n",
    "Ada_w2v =  Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), (\"Ada\", AdaBoostClassifier(n_estimators=10))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('data/df_category_scores.json') as f:    \n",
    "        scores = json.load(f)\n",
    "except:\n",
    "    scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    pred_proba = pd.read_pickle('data/df_category_pred_proba.pkl')\n",
    "except:\n",
    "    pred_proba = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 989: Pinot Noir\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "target = open('data/training_output.txt', 'w')\n",
    "print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "        \n",
    "\n",
    "for key in category_positive_samples:\n",
    "    if str(key) in scores:\n",
    "        continue\n",
    "    if key in scores:\n",
    "        continue\n",
    "    else:\n",
    "        find_name = df_catergory_lookup.category_id == key\n",
    "        name = df_catergory_lookup[find_name].category_name\n",
    "        the_index = name.index[0]\n",
    "\n",
    "        target.write(\"processing {0}: {1}\\n\".format(key, name.ix[the_index]))\n",
    "        category_positive_samples[key].loc[:,('is_category')] = 1\n",
    "        category_negative_samples[key].loc[:,('is_category')] = 0\n",
    "        \n",
    "        # Split into training and testing set\n",
    "        data = category_positive_samples[key].append(category_negative_samples[key], ignore_index=True)\n",
    "        X = data['item_labels']\n",
    "        y = data['is_category']\n",
    "        target.write('{:5.3f}% positive samples\\n'.format(len(category_positive_samples[key])/len(X)))\n",
    "        target.write(\"Number of datapoints in set {}\\n\".format(len(X)))\n",
    "\n",
    "        target.write('Training the model....\\n')\n",
    "\n",
    "        scores[key] = cross_val_score(Ada_w2v,  X, y, cv=5, scoring='f1').mean()\n",
    "        Ada_w2v.fit(X, y)\n",
    "        pred_proba[key] = Ada_w2v.predict_proba(X)\n",
    "\n",
    "        print('Finished training category{0}: {1}\\n\\n'.format(key, name.ix[the_index]))\n",
    "        target.write('Finished training category{0}: {1}\\n\\n'.format(key, name.ix[the_index]))\n",
    "        \n",
    "target.close()    \n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "json.dump(scores, open(\"data/df_category_scores.json\",'w'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 14 hours to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get *k* vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in category_positive_samples:\n",
    "    df_ = category_positive_samples[key].append(category_negative_samples[key])\n",
    "    scores_ = pd.DataFrame(pred_proba[key], columns = ['Neg','catg_'+str(key)])\n",
    "    scores_.drop(['Neg'], axis = 1, inplace = True)\n",
    "    scores_ = scores_.set_index(df_.index)\n",
    "    df.ix[df_.index, 'catg_'+str(key)] = scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write vectors to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('data/df_data_vectors.dat')\n",
    "pickle.dump(pred_proba, open(\"data/df_category_pred_proba.pkl\",'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Architecture\n",
    "<figure>\n",
    "  <img src=\"images/hc_4.png\" style=\"width: 450px;\">\n",
    "</figure>\n",
    "After training the *k=141* classifiers, I extracted the *k* vectors; I carefully combined them with the engineered features, and the other raw metadata taking care to ensure that I assigned the right probabilities to the right data points in my training set. I feed these features and labels into a multi-label, multi-class AdaBoost classifier. \n",
    "\n",
    "I feed these features and labels into a multi-label, multi-class AdaBoost classifier. I took 75% of the data for training and 25% for testing. Once again I do a 5 fold cross-validation scheme, fit the final classifier, and retrieve the predicted class probabilities.\n",
    "\n",
    "\n",
    "#### Get Data for Final Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "the_indices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in category_positive_samples:\n",
    "    key_df = df['label'] == key\n",
    "    the_indices[key] = df[key_df].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in the_indices:\n",
    "    X = X.append(pd.DataFrame(df.loc[the_indices[key]], columns = df.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.drop([u'mapped_category', u'category',\n",
    "u'item_name',         u'level_0',\n",
    "u'level_1',         u'level_2',         u'level_3',\n",
    "u'level_4',         u'level_5',         u'level_6',\n",
    "u'mapped_level_0',  u'mapped_level_1',  u'mapped_level_2',\n",
    "u'mapped_level_3',  u'mapped_level_4',  u'mapped_level_5',\n",
    "u'mapped_level_6',   u'item_labels'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all rows whose label is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X[X['label'].notnull()]\n",
    "X = X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = X['label']\n",
    "X.drop(['label'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of datapoints in set {}\".format(len(X)))\n",
    "# First, decide how many training vs test samples you want\n",
    "num_train = ( len(y) // 4) *3\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "try:\n",
    "    X_train, y_train, X_test, y_test = shuffle_split_data(X, y)\n",
    "    print (\"Successfully shuffled and split the data!\")\n",
    "except:\n",
    "    print (\"Something went wrong with shuffling and splitting the data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "          'XGBoost': XGBClassifier(),\n",
    "          'DecisionTree': DecisionTreeClassifier(random_state=seed),\n",
    "          'SVC': LinearSVC(random_state=seed),\n",
    "          'RandomForest': RandomForestClassifier(random_state=seed),\n",
    "          'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=seed)\n",
    "         }\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_transform = scaler.fit_transform(X_train)\n",
    "\n",
    "print('CLASSIFICATION RESULTS OF BASELINE CLASSIFIERS\\n')\n",
    "print('{:20}{:^15}{:^10}'.format('CLASSIFIER', 'MEAN SCORE %', 'STD DEV %'))\n",
    "\n",
    "\n",
    "for clf_name, clf in models.iteritems():\n",
    "    results = cross_val_score(clf, X_transform, y_train, cv=5)\n",
    "    print('{:20}{:^15.2f}{:^10.2f}'.format(clf_name, results.mean()*100, results.std()*100) ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "clf = XGBClassifier()\n",
    "clf.fit(X_train_trans, y_train)\n",
    "X_test_trans = scaler.transform(X_test)\n",
    "final_preds = clf.predict(X_test_trans)\n",
    "precision, recall, fbeta_score, support = score(y_test, final_preds)\n",
    "\n",
    "\n",
    "print (\"Precision: {:15.3f}\\nRecall: {:^10.3f}\\nF Score{:^10.3f}\\nSupport:{:^10.3f}\".format(precision.mean()*100, \n",
    "                                                 recall.mean()*100, fbeta_score.mean()*100, len(support)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score(y_test, final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_test, final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (clf.classes_)\n",
    "C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = df_catergory_lookup['category_id'] == 263\n",
    "df_catergory_lookup[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Result\n",
    "With the understanding that chance is 1/141 * 100 = 0.7%, my approach achieves a Precision of 17%, Recall of 19% and FScore of 15% for the last level of the tree. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in category_positive_samples:\n",
    "        find_name = df_catergory_lookup.category_id == key\n",
    "        name = df_catergory_lookup[find_name].category_name\n",
    "        the_index = name.index[0]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outX = pd.DataFrame(index=y_test.index)\n",
    "outX = outX.join(y_test)\n",
    "outX['predicted'] = final_preds\n",
    "outX['predicted_name'] = ''\n",
    "outX['item_name'] = pd.DataFrame(df.loc[outX.index])['item_name']\n",
    "outX['mapped_level_0'] = pd.DataFrame(df.loc[outX.index])['mapped_level_0']\n",
    "outX['level_0'] = pd.DataFrame(df.loc[outX.index])['level_0']\n",
    "outX['mapped_level_1'] = pd.DataFrame(df.loc[outX.index])['mapped_level_1']\n",
    "outX['level_1'] = pd.DataFrame(df.loc[outX.index])['level_1']\n",
    "outX['mapped_level_2'] = pd.DataFrame(df.loc[outX.index])['mapped_level_2']\n",
    "outX['level_2'] = pd.DataFrame(df.loc[outX.index])['level_2']\n",
    "outX['mapped_level_3'] = pd.DataFrame(df.loc[outX.index])['mapped_level_3']\n",
    "outX['level_3'] = pd.DataFrame(df.loc[outX.index])['level_3']\n",
    "outX['mapped_level_4'] = pd.DataFrame(df.loc[outX.index])['mapped_level_4']\n",
    "outX['level_4'] = pd.DataFrame(df.loc[outX.index])['level_4']\n",
    "outX['mapped_level_5'] = pd.DataFrame(df.loc[outX.index])['mapped_level_5']\n",
    "outX['level_5'] = pd.DataFrame(df.loc[outX.index])['level_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in final_preds:\n",
    "    find_name = df_catergory_lookup.category_id == i\n",
    "    outX.ix[outX.index[j], 'predicted_name'] = df_catergory_lookup[find_name]['category_name'].values[0]\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#score(y_test, final_preds)\n",
    "outX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a category find ancestors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification error by tree depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_lookup = df[['level_0', 'level_1', 'level_2', 'level_3', 'level_4', 'level_5',\n",
    "                        'mapped_level_0', 'mapped_level_1', 'mapped_level_2', 'mapped_level_3',\n",
    "                        'mapped_level_4', 'mapped_level_5']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key = 18\n",
    "a = (df_lookup['level_0'] == key)\n",
    "b = (df_lookup['level_1'] == key)\n",
    "c = (df_lookup['level_2'] == key)\n",
    "d = (df_lookup['level_3'] == key)\n",
    "e = (df_lookup['level_3'] == key)\n",
    "df_lookup[ a | b | c | d | e ][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_positive_samples[235].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain classifier for category k = 235, chocolate\n",
    "The value of this method is that I can take a look at the results, determine which individual text-based classifiers have poor results, and optimize and tune that specific classifier. In the case of the chocolate classifier, I see that the minority class is minuscule. To deal with this great imbalance in the data, I synthetically resample it and retrain the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "key = 235\n",
    "\n",
    "data = category_positive_samples[key].append(category_negative_samples[key], ignore_index=True)\n",
    "X_235 = data['item_labels']\n",
    "y_235 = data['is_category']\n",
    "        \n",
    "vectorizer = MeanEmbeddingVectorizer(w2v)\n",
    "vectorizer.fit(X_235, y_235)  \n",
    "\n",
    "sm = SMOTE(random_state=seed)\n",
    "\n",
    "print('Original dataset shape {}'.format(Counter(y_235)))\n",
    "X_res, y_res = sm.fit_sample(vectorizer.transform(X_235) , y_235)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Training the model....')\n",
    "clf = AdaBoostClassifier(n_estimators=10)\n",
    "scores_235 = cross_val_score(clf, X_res, y_res, cv=5, scoring='f1').mean()\n",
    "clf.fit(X_res, y_res)\n",
    "pred_proba_235 = clf.predict_proba(X_res)\n",
    "print('Finished training category\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = vectorizer.transform(X_235)\n",
    "clf.fit(X_, y_235)\n",
    "pred_proba_235 = clf.predict_proba(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores[key] = scores_235\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key = 235\n",
    "df_ = category_positive_samples[key].append(category_negative_samples[key])\n",
    "scores_ = pd.DataFrame(pred_proba_235, columns = ['Neg','catg_'+str(key)])\n",
    "scores_.drop(['Neg'], axis = 1, inplace = True)\n",
    "scores_ = scores_.set_index(df_.index)\n",
    "df.ix[df_.index, 'catg_'+str(key)] = scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate chance and scores for each level in the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
